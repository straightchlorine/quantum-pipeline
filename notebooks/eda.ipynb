{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQE Data Exploration & Feature Engineering\n",
    "\n",
    "**Roadmap item #7** — Understand the ~38k VQE iterations collected during the thesis.\n",
    "\n",
    "Goals:\n",
    "1. Assess data quality and completeness\n",
    "2. Understand distributions across molecules, basis sets, and backends\n",
    "3. Identify predictive features for ML targets (#8 convergence, #9 energy, #10 optimizer)\n",
    "4. Engineer trajectory-based features from per-iteration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "DATA_DIR = Path(\"../data\")  # adjust to actual location of full parquet export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load the full dataset — either from re-exported parquet (with the 3 new fields:\n",
    "`energy_delta`, `parameter_delta_norm`, `cumulative_min_energy`) or from the\n",
    "Iceberg tables via Spark.\n",
    "\n",
    "Key tables:\n",
    "- `vqe_results` — one row per experiment (summary)\n",
    "- `vqe_iterations` — one row per iteration per experiment (trajectory)\n",
    "- `iteration_parameters` — parameter vectors per iteration\n",
    "- `molecules` — molecular geometry info\n",
    "- `performance_metrics` — timing breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with actual data paths once full export is available\n",
    "# df_results = pd.read_parquet(DATA_DIR / \"vqe_results\")\n",
    "# df_iterations = pd.read_parquet(DATA_DIR / \"vqe_iterations\")\n",
    "# df_molecules = pd.read_parquet(DATA_DIR / \"molecules\")\n",
    "# df_metrics = pd.read_parquet(DATA_DIR / \"performance_metrics\")\n",
    "\n",
    "# Temporary: load the sample file\n",
    "df_results = pd.read_parquet(\"../examples/00000-104-1ca35fc5-818f-40ac-aec9-6b72819d59fe-00001.parquet\")\n",
    "print(f\"Results shape: {df_results.shape}\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema & Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- dtypes ---\")\n",
    "print(df_results.dtypes)\n",
    "print(f\"\\n--- nulls ---\")\n",
    "print(df_results.isnull().sum())\n",
    "print(f\"\\n--- unique experiments: {df_results['experiment_id'].nunique()} ---\")\n",
    "print(f\"\\n--- row counts per molecule_id ---\")\n",
    "print(df_results.groupby(\"molecule_id\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for suspicious data\n",
    "print(\"Runs with total_iterations <= 1:\", (df_results[\"total_iterations\"] <= 1).sum())\n",
    "print(\"Runs with NaN energy:\", df_results[\"minimum_energy\"].isna().sum())\n",
    "print(\"Duplicate experiment_ids:\", df_results[\"experiment_id\"].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Univariate Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "df_results[\"total_iterations\"].hist(bins=50, ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Total Iterations Distribution\")\n",
    "axes[0, 0].set_xlabel(\"iterations\")\n",
    "\n",
    "df_results[\"minimum_energy\"].hist(bins=50, ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Minimum Energy Distribution\")\n",
    "axes[0, 1].set_xlabel(\"energy (Ha)\")\n",
    "\n",
    "df_results[\"num_qubits\"].value_counts().sort_index().plot.bar(ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Experiments per Qubit Count\")\n",
    "\n",
    "df_results[\"basis_set\"].value_counts().plot.bar(ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Experiments per Basis Set\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bivariate Analysis\n",
    "\n",
    "Key questions:\n",
    "- How does convergence (total_iterations) vary by molecule/basis set?\n",
    "- Is there a relationship between num_qubits and iterations needed?\n",
    "- Do different optimizers produce significantly different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(data=df_results, x=\"basis_set\", y=\"total_iterations\", ax=axes[0])\n",
    "axes[0].set_title(\"Iterations by Basis Set\")\n",
    "\n",
    "sns.scatterplot(data=df_results, x=\"num_qubits\", y=\"total_iterations\",\n",
    "                hue=\"basis_set\", alpha=0.6, ax=axes[1])\n",
    "axes[1].set_title(\"Iterations vs Qubit Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy by molecule and basis set\n",
    "pivot = df_results.groupby([\"molecule_id\", \"basis_set\"])[\"minimum_energy\"].agg([\"mean\", \"std\", \"count\"])\n",
    "print(pivot.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer comparison (if multiple optimizers exist in data)\n",
    "if df_results[\"optimizer\"].nunique() > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    sns.boxplot(data=df_results, x=\"optimizer\", y=\"total_iterations\", ax=ax)\n",
    "    ax.set_title(\"Iterations by Optimizer\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Only one optimizer in dataset: {df_results['optimizer'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Iteration Trajectory Analysis\n",
    "\n",
    "Per-iteration energy curves — this is where the real ML signal lives.\n",
    "Requires `vqe_iterations` table with the new fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment when full iteration data is available\n",
    "# df_iter = df_iterations.copy()\n",
    "\n",
    "# # Plot energy trajectories for a sample of experiments\n",
    "# sample_ids = df_results.sample(min(10, len(df_results)))[\"experiment_id\"].values\n",
    "# fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# for eid in sample_ids:\n",
    "#     traj = df_iter[df_iter[\"experiment_id\"] == eid].sort_values(\"iteration\")\n",
    "#     ax.plot(traj[\"iteration\"], traj[\"result\"], alpha=0.7, label=eid[:8])\n",
    "# ax.set_xlabel(\"Iteration\")\n",
    "# ax.set_ylabel(\"Energy (Ha)\")\n",
    "# ax.set_title(\"Sample Energy Trajectories\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: energy_delta and parameter_delta_norm distributions\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "#\n",
    "# df_iter[\"energy_delta\"].dropna().hist(bins=100, ax=axes[0])\n",
    "# axes[0].set_title(\"Energy Delta Distribution\")\n",
    "#\n",
    "# df_iter[\"parameter_delta_norm\"].dropna().hist(bins=100, ax=axes[1])\n",
    "# axes[1].set_title(\"Parameter Delta Norm Distribution\")\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "Derive features from trajectory data for ML models:\n",
    "\n",
    "| Feature | Source | ML Target |\n",
    "|---------|--------|-----------|\n",
    "| `slope_first_n` | Linear fit on first N iteration energies | #8 convergence, #9 energy |\n",
    "| `energy_variance_first_n` | Variance of first N energies | #8 convergence |\n",
    "| `improvement_rate` | Mean energy_delta over first N | #8, #9 |\n",
    "| `param_stability` | Std of parameter_delta_norm | #8 convergence |\n",
    "| `converged` | Binary: did run hit threshold? | #8 convergence |\n",
    "| `iterations_to_converge` | Iteration where energy_delta < threshold | #8 convergence |\n",
    "| `energy_gap` | Final energy - known ground state | #9 energy |\n",
    "| `qubit_count` | num_qubits | All |\n",
    "| `hamiltonian_size` | From hamiltonian_terms table | All |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERGENCE_THRESHOLD = 1e-6  # Ha, from pipeline config\n",
    "EARLY_WINDOW = 10  # first N iterations for trajectory features\n",
    "\n",
    "\n",
    "def extract_trajectory_features(traj_df, experiment_id, window=EARLY_WINDOW):\n",
    "    \"\"\"Extract ML features from an experiment's iteration trajectory.\"\"\"\n",
    "    traj = traj_df[traj_df[\"experiment_id\"] == experiment_id].sort_values(\"iteration\")\n",
    "    early = traj.head(window)\n",
    "\n",
    "    features = {\"experiment_id\": experiment_id}\n",
    "\n",
    "    # Early trajectory slope (linear fit)\n",
    "    if len(early) >= 2:\n",
    "        coeffs = np.polyfit(early[\"iteration\"], early[\"result\"], 1)\n",
    "        features[\"slope_first_n\"] = coeffs[0]\n",
    "    else:\n",
    "        features[\"slope_first_n\"] = np.nan\n",
    "\n",
    "    # Energy variance in early window\n",
    "    features[\"energy_variance_first_n\"] = early[\"result\"].var()\n",
    "\n",
    "    # Mean improvement rate\n",
    "    if \"energy_delta\" in early.columns:\n",
    "        features[\"mean_energy_delta\"] = early[\"energy_delta\"].dropna().mean()\n",
    "\n",
    "    # Parameter stability\n",
    "    if \"parameter_delta_norm\" in early.columns:\n",
    "        features[\"param_stability\"] = early[\"parameter_delta_norm\"].dropna().std()\n",
    "\n",
    "    # Convergence label\n",
    "    if \"energy_delta\" in traj.columns:\n",
    "        converged_mask = traj[\"energy_delta\"].abs() < CONVERGENCE_THRESHOLD\n",
    "        features[\"converged\"] = converged_mask.any()\n",
    "        if converged_mask.any():\n",
    "            features[\"iterations_to_converge\"] = traj.loc[converged_mask.idxmax(), \"iteration\"]\n",
    "        else:\n",
    "            features[\"iterations_to_converge\"] = np.nan\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# TODO: apply to full iteration dataset\n",
    "# feature_rows = [extract_trajectory_features(df_iter, eid) for eid in df_results[\"experiment_id\"]]\n",
    "# df_features = pd.DataFrame(feature_rows)\n",
    "# df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Matrix & ML Viability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation on summary-level numeric columns\n",
    "numeric_cols = df_results.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_cols) > 2:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(df_results[numeric_cols].corr(), annot=True, cmap=\"coolwarm\",\n",
    "                center=0, fmt=\".2f\", ax=ax)\n",
    "    ax.set_title(\"Feature Correlations (Summary Level)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: full correlation with engineered features\n",
    "# df_full = df_results.merge(df_features, on=\"experiment_id\")\n",
    "# target_cols = [\"total_iterations\", \"minimum_energy\", \"converged\"]\n",
    "# feature_cols = [\"num_qubits\", \"ansatz_reps\", \"slope_first_n\",\n",
    "#                 \"energy_variance_first_n\", \"mean_energy_delta\", \"param_stability\"]\n",
    "# print(df_full[feature_cols + target_cols].corr()[target_cols].drop(target_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "**TODO** — Fill in after running with full data:\n",
    "\n",
    "- [ ] Data quality: any issues found?\n",
    "- [ ] Which ML targets are viable?\n",
    "  - Convergence prediction (#8): viable if ...\n",
    "  - Energy estimation (#9): viable if ...\n",
    "  - Optimizer recommendation (#10): viable if multiple optimizers in data\n",
    "- [ ] Key predictive features identified\n",
    "- [ ] Recommended next steps for #8 and #9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}