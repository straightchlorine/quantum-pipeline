{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115ffcad-1751-45ce-bbf5-02c5efcfa6c1",
   "metadata": {},
   "source": [
    "# SparkSession initialization and MinIO/S3 access configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16dd501-1ee8-4946-9928-5ba52eb3c4d0",
   "metadata": {},
   "source": [
    "Static variables' definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a948dfa-dffb-4a3f-ae4e-8bde5e9c55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3 = 'http://server:9000'\n",
    "SPARK_MASTER = \"spark://server:7077\"\n",
    "HOST_IP = \"station\"\n",
    "S3_BUCKET = 's3a://local-vqe-results/experiments/'\n",
    "S3_WAREHOUSE = 's3a://local-features/warehouse/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e724e4-c528-4e55-8291-bfcd7a49e163",
   "metadata": {},
   "source": [
    "Access to MinIO/S3 is based on `MINIO_ACCESSS_KEY` and `MINIO_SECRET_KEY` its important for both values to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a50181-63df-4792-a17b-0b3a86f6f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.environ.get('MINIO_ACCESS_KEY') and not os.environ.get('MINIO_SECRET_KEY'):\n",
    "    raise ValueError('Both MINIO_ACCESS_KEY and MINIO_SECRET_KEY are not set.')\n",
    "if not os.environ.get('MINIO_ACCESS_KEY') and os.environ.get('MINIO_SECRET_KEY'):\n",
    "    raise ValueError('MINIO_ACCESS_KEY is not set.')\n",
    "if os.environ.get('MINIO_ACCESS_KEY') and not os.environ.get('MINIO_SECRET_KEY'):\n",
    "    raise ValueError('MINIO_SECRET_KEY is not set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4d87fa-455f-403d-af87-cd2343c5ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/zweiss/.pyenv/versions/3.12.9/envs/quantum-pipeline/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/zweiss/.ivy2/cache\n",
      "The jars for the packages stored in: /home/zweiss/.ivy2/jars\n",
      "org.slf4j#slf4j-api added as a dependency\n",
      "commons-codec#commons-codec added as a dependency\n",
      "com.google.j2objc#j2objc-annotations added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c1c1aaed-5f20-42a6-a7fe-b1893ab793e4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound commons-codec#commons-codec;1.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.5 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.40.v20210413 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.7 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.30 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.1 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.2 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.2 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.10.5.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.10.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.10.5 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.2 in central\n",
      ":: resolution report :: resolve 873ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.10.5.1 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;3.0.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.18.0 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.2 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.2 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.7 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.2 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.5 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.40.v20210413 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.30 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 by [com.google.j2objc#j2objc-annotations;3.0.0] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.18.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;2.0.17] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  102  |   0   |   0   |   3   ||   99  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c1c1aaed-5f20-42a6-a7fe-b1893ab793e4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 99 already retrieved (0kB/14ms)\n",
      "25/04/14 17:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('Quantum Pipeline Feature Processing')\n",
    "    .master(SPARK_MASTER)\n",
    "    .config(\"spark.driver.host\", HOST_IP)\n",
    "    .config(\"spark.jars.packages\", (\n",
    "        'org.slf4j:slf4j-api:2.0.17,'\n",
    "        'commons-codec:commons-codec:1.18.0,'\n",
    "        'com.google.j2objc:j2objc-annotations:3.0.0,'\n",
    "        'org.apache.spark:spark-avro_2.12:3.5.5,'\n",
    "        'org.apache.hadoop:hadoop-aws:3.3.1,'\n",
    "        'org.apache.hadoop:hadoop-common:3.3.1,'\n",
    "        'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,'\n",
    "        )\n",
    "    )\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.quantum_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.quantum_catalog.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.quantum_catalog.warehouse\", S3_WAREHOUSE)\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get('MINIO_ACCESS_KEY'))\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get('MINIO_SECRET_KEY'))\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e0839-d8db-4df7-a04e-084c79e133e7",
   "metadata": {},
   "source": [
    "# Reading the data from MinIO/S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462009a-8fae-4098-80d5-4ba94c227c26",
   "metadata": {},
   "source": [
    "Read available topics from the MinIO/S3 - different experiments are in different topics i.e.:\n",
    "\n",
    "`vqe_decorated_result_mol0_HH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu` \n",
    "\n",
    "This topic defines an experiment on molecule with id 0, which is H_2, containing a single iteration of the VQE, with basis set of sto3g and backend utilising GPU acceleration and statevector simulation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5210a0-4b4b-4fc6-8a48-70f5be345d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def list_available_topics():\n",
    "    \"\"\"List available topics within experiments location from the storage.\"\"\"\n",
    "    # configure hadoop to use appropriate filesystem\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \n",
    "                                         \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    \n",
    "    # create a configured filesystem\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark._jvm.java.net.URI.create(S3_BUCKET), \n",
    "        spark._jsc.hadoopConfiguration()\n",
    "    )\n",
    "    \n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(S3_BUCKET)\n",
    "    \n",
    "    try:\n",
    "        if fs.exists(path) and fs.isDirectory(path):\n",
    "            return [f.getPath().getName() for f in fs.listStatus(path) if f.isDirectory()]\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing S3: {e}\")\n",
    "        return []\n",
    "\n",
    "def read_experiments_by_topic(topic_name):\n",
    "    \"\"\"Read Avro experiment files from a specific topic's directory.\"\"\"\n",
    "    topic_path = f\"{S3_BUCKET}{topic_name}/partition=*/*.avro\"\n",
    "    df = spark.read.format(\"avro\").load(topic_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467684a6-3c48-4ebb-bf88-4aa2fae5d898",
   "metadata": {},
   "source": [
    "Create a dataframe out of experiment data files from the storage. For currently the only implemented algorithm - VQE - output of `df.show(1)` looks like so:\n",
    "```\n",
    "+--------------------+--------------------+---------+------------------+-------------------+------------------+------------------+-----------+\n",
    "|          vqe_result|            molecule|basis_set|  hamiltonian_time|       mapping_time|          vqe_time|        total_time|molecule_id|\n",
    "+--------------------+--------------------+---------+------------------+-------------------+------------------+------------------+-----------+\n",
    "|{{aer_simulator_s...|{{[H, H], [[0.0, ...|    sto3g|0.4875204563140869|0.02751922607421875|0.4407076835632324|0.9557473659515381|          0|\n",
    "+--------------------+--------------------+---------+------------------+-------------------+------------------+------------------+-----------+\n",
    "only showing top 1 row\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd2a118-eaa1-4510-a3a5-4b5947d38196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 17:09:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Topics: ['vqe_decorated_result_mol0_HH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol1_OHH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol2_HeHe_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol3_LiH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol4_BeH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol5_BH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol6_CHHHH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol7_NHHH_it1_bs_sto3g_bk_aer_simulator_statevector_gpu', 'vqe_decorated_result_mol8_NN_it1_bs_sto3g_bk_aer_simulator_statevector_gpu']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------------------+--------------------+-------------------+-------------------+-----------+\n",
      "|          vqe_result|            molecule|basis_set|  hamiltonian_time|        mapping_time|           vqe_time|         total_time|molecule_id|\n",
      "+--------------------+--------------------+---------+------------------+--------------------+-------------------+-------------------+-----------+\n",
      "|{{aer_simulator_s...|{{[H, H], [[0.0, ...|    sto3g|0.0638587474822998|0.022032976150512695|0.33730435371398926|0.42319607734680176|          0|\n",
      "+--------------------+--------------------+---------+------------------+--------------------+-------------------+-------------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 17:12:43 ERROR TaskSchedulerImpl: Lost executor 0 on 172.22.0.6: worker lost: 172.22.0.6:34979 got disassociated\n",
      "25/04/14 17:12:53 WARN StandaloneAppClient$ClientEndpoint: Connection to 640653264d32:7077 failed; waiting for master to reconnect...\n",
      "25/04/14 17:12:53 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n"
     ]
    }
   ],
   "source": [
    "# get topics available in the storage\n",
    "available_topics = list_available_topics()\n",
    "print(\"Available Topics:\", available_topics)\n",
    "\n",
    "# read one particular topic\n",
    "df = read_experiments_by_topic(available_topics[0]).repartition(1)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbb577-0384-4565-aaf5-413472547bb9",
   "metadata": {},
   "source": [
    "# Processing the data into ML features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62be90c-612a-4dc8-a015-f655cf41cb43",
   "metadata": {},
   "source": [
    "#### In order for data tracking to be as efficient as possible metadata columns are added for each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e999010-4208-46e9-a7de-f52dcd5e6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, current_timestamp, current_date, lit, col, exp, size, explode, expr\n",
    "from pyspark.sql.types import StringType\n",
    "import uuid\n",
    "\n",
    "def add_metadata_columns(dataframe, processing_name):\n",
    "    \"\"\"Add metadata columns to the dataframes.\"\"\"\n",
    "    #  udf to generate uuids\n",
    "    generate_uuid = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "    \n",
    "    return dataframe \\\n",
    "        .withColumn(\"experiment_id\", generate_uuid()) \\\n",
    "        .withColumn(\"processing_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"processing_date\", current_date()) \\\n",
    "        .withColumn(\"processing_batch_id\", lit(str(uuid.uuid4()))) \\\n",
    "        .withColumn(\"processing_name\", lit(processing_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5db917-28d9-484d-b6e8-4907ba3fcb6e",
   "metadata": {},
   "source": [
    "#### Here Iceberg table is created with specified parameters i.e. partitioning in order to group similar rows together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ee1f1-c5ef-4be4-acd7-d36f878213c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iceberg_table(dataframe, table_name, partition_columns=None, comment=None):\n",
    "    \"\"\"Create Iceberg table with partitioning and other properties (if provided).\"\"\"\n",
    "    writer = dataframe.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .option(\"write-format\", \"parquet\")\n",
    "    \n",
    "    # partition, if specified partition columns\n",
    "    if partition_columns:\n",
    "        writer = writer.partitionBy(*partition_columns)\n",
    "    \n",
    "    # add optional properties, if defined\n",
    "    if comment:\n",
    "        writer = writer.option(\"comment\", comment)\n",
    "\n",
    "    # create the table for the feature\n",
    "    writer.mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"quantum_catalog.quantum_features.{table_name}\")\n",
    "    \n",
    "    print(f\"Created table: quantum_catalog.quantum_features.{table_name}\")\n",
    "    \n",
    "    # create a tag for this version of the table\n",
    "    snapshot_id = spark.sql(f\"SELECT snapshot_id FROM quantum_catalog.quantum_features.{table_name}.snapshots ORDER BY committed_at DESC LIMIT 1\").collect()[0][0]\n",
    "    version_tag = f\"v_{dataframe.first().processing_batch_id.replace('-', '')}\"\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE quantum_catalog.quantum_features.{table_name}\n",
    "    CREATE TAG {version_tag} AS OF VERSION {snapshot_id}\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"Created version tag: {version_tag} for table {table_name}\")\n",
    "    \n",
    "    return version_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7acbb3-2f34-48e9-ab51-29c69eb6a686",
   "metadata": {},
   "source": [
    "#### Key part of the incremental updates - function checks if provided table exists and if it does, it utilises an antijoin to get only the records that do not exist in the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123542e-c680-4e10-8ab7-5de5f6cdbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_new_records(new_data_df, table_name, key_columns):\n",
    "    \"\"\"\n",
    "    Identifies records in new_data_df that don't exist in the target table.\n",
    "    Uses DataFrame operations for better performance.\n",
    "    \n",
    "    Args:\n",
    "        new_data_df: DataFrame containing potentially new data\n",
    "        table_name: Name of the table to check against\n",
    "        key_columns: List of column names that uniquely identify records\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: records that don't exist in the target table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # if new_data_df empty, return empty df\n",
    "        if new_data_df.isEmpty():\n",
    "            return new_data_df\n",
    "\n",
    "        # if table does not exist, every record is new\n",
    "        if not spark.catalog.tableExists(f\"quantum_catalog.quantum_features.{table_name}\"):\n",
    "            return new_data_df\n",
    "\n",
    "        # retrieve existing keys\n",
    "        existing_keys = spark.sql(f\"SELECT DISTINCT {', '.join(key_columns)} FROM quantum_catalog.quantum_features.{table_name}\")\n",
    "        \n",
    "        # if table exists, but has no records - return all records\n",
    "        if existing_keys.isEmpty():\n",
    "            return new_data_df\n",
    "\n",
    "        # marker column for the join\n",
    "        new_with_marker = new_data_df.select(*key_columns).distinct().withColumn(\"is_new\", lit(1))\n",
    "        existing_with_marker = existing_keys.withColumn(\"exists\", lit(1))\n",
    "\n",
    "        # left join\n",
    "        joined = new_with_marker.join(existing_with_marker, on=key_columns, how=\"left\")\n",
    "\n",
    "        # ensure join is not empty\n",
    "        if joined.isEmpty():\n",
    "            return new_data_df\n",
    "\n",
    "        # get only the keys that do not exist in the records yet\n",
    "        new_keys = joined.filter(col(\"exists\").isNull()).select(*key_columns)\n",
    "\n",
    "        # ensure new keys are not empty\n",
    "        if new_keys.isEmpty():\n",
    "            return new_data_df.limit(0)\n",
    "\n",
    "        # join back to get an array with full records\n",
    "        truly_new_data = new_data_df.join(new_keys, on=key_columns, how=\"inner\")\n",
    "\n",
    "        return truly_new_data\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error identifying new records: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9356c0c-36d5-4bc6-a36d-727dca79d430",
   "metadata": {},
   "source": [
    "#### Incremental update i.e. processes only those features that weren't processed before. If the table exists new data is appended to existing features, otherwise creates the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32c9fc-bd4d-4b2b-95c2-7649e1faf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_incremental_data(new_data_df, table_name, key_columns, partition_columns=None, comment=None):\n",
    "    \"\"\"\n",
    "    Process the data incrementally - only data that wasn't processed yet.\n",
    "    \n",
    "    Args:\n",
    "      new_data_df: DataFrame containing potentially new data\n",
    "      table_name: Name of the target table\n",
    "      key_columns: List of column names that uniquely identify records\n",
    "      partition_columns: Optional list of columns to partition by\n",
    "      comment: Optional comment for the table\n",
    "    \n",
    "    Returns:\n",
    "        tuple:  version tag and count of new records processed\n",
    "    \"\"\"\n",
    "    # check if table exists\n",
    "    table_exists = spark.catalog._jcatalog.tableExists(f\"quantum_catalog.quantum_features.{table_name}\")\n",
    "\n",
    "    # if the table doesn't exist, create it\n",
    "    if not table_exists:\n",
    "        writer = new_data_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .option(\"write-format\", \"parquet\")\n",
    "        \n",
    "        # partition, if specified partition columns\n",
    "        if partition_columns:\n",
    "            writer = writer.partitionBy(*partition_columns)\n",
    "        \n",
    "        # add optional properties, if defined\n",
    "        if comment:\n",
    "            writer = writer.option(\"comment\", comment)\n",
    "\n",
    "        # create the table for the feature\n",
    "        writer.mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"quantum_catalog.quantum_features.{table_name}\")\n",
    "        \n",
    "        print(f\"Created table: quantum_catalog.quantum_features.{table_name}\")\n",
    "        \n",
    "        # create a tag for this version of the table\n",
    "        snapshot_id = spark.sql(f\"SELECT snapshot_id FROM quantum_catalog.quantum_features.{table_name}.snapshots ORDER BY committed_at DESC LIMIT 1\").collect()[0][0]\n",
    "        \n",
    "        # Get processing_batch_id before we lose reference to the DataFrame\n",
    "        first_row = new_data_df.limit(1).collect()\n",
    "        processing_batch_id = first_row[0][\"processing_batch_id\"].replace('-', '')\n",
    "        version_tag = f\"v_{processing_batch_id}\"\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "        ALTER TABLE quantum_catalog.quantum_features.{table_name}\n",
    "        CREATE TAG {version_tag} AS OF VERSION {snapshot_id}\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"Created version tag: {version_tag} for table {table_name}\")\n",
    "        \n",
    "        return version_tag, new_data_df.count()\n",
    "    else:\n",
    "        # if table does exist\n",
    "        # Store the processing_batch_id before filtering for new records\n",
    "        first_row = new_data_df.limit(1).collect()\n",
    "        if not first_row:\n",
    "            print(f\"Input dataset is empty for {table_name}\")\n",
    "            return None, 0\n",
    "            \n",
    "        processing_batch_id = first_row[0][\"processing_batch_id\"].replace(\"-\", \"\")\n",
    "        \n",
    "        truly_new_data = identify_new_records(new_data_df, table_name, key_columns)\n",
    "\n",
    "        # if no new data, do not move with the process\n",
    "        new_record_count = truly_new_data.count()\n",
    "        if new_record_count == 0:\n",
    "            print(f\"No new records found for table {table_name}\")\n",
    "            return None, 0\n",
    "\n",
    "        # write only new data to the Iceberg\n",
    "        writer = truly_new_data.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .option(\"write-format\", \"parquet\")\n",
    "    \n",
    "        # add partitioning if specified\n",
    "        if partition_columns:\n",
    "            writer = writer.partitionBy(*partition_columns)\n",
    "    \n",
    "        # append to the existing table\n",
    "        writer.mode(\"append\") \\\n",
    "            .saveAsTable(f\"quantum_catalog.quantum_features.{table_name}\")\n",
    "    \n",
    "        print(f\"Appended {new_record_count} new records to table {table_name}\")\n",
    "        \n",
    "        # create a tag for the incremental update\n",
    "        snapshot_id = spark.sql(\n",
    "            f\"SELECT snapshot_id FROM quantum_catalog.quantum_features.{table_name}.snapshots ORDER BY committed_at DESC LIMIT 1\"\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        version_tag = f\"v_incr_{processing_batch_id}\"\n",
    "    \n",
    "        spark.sql(f'''\n",
    "            ALTER TABLE quantum_catalog.quantum_features.{table_name}\n",
    "            CREATE TAG {version_tag} AS OF VERSION {snapshot_id}\n",
    "        ''')\n",
    "    \n",
    "        print(f\"Created incremental version tag: {version_tag} for table {table_name}\")\n",
    "    \n",
    "        return version_tag, new_record_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbc610-3666-4442-8c00-7c944eda2bcd",
   "metadata": {},
   "source": [
    "#### Unpack the data into DataFrames and add unique fields to identify each experiment and iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee0362-934e-4947-8272-025c3fa0ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_quantum_data(df):\n",
    "    \"\"\"\n",
    "    Transforms the original quantum data into various feature tables.\n",
    "    \n",
    "    Args:\n",
    "        df: Original dataframe with quantum simulation data\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of transformed dataframes\n",
    "    \"\"\"\n",
    "    # original dataframe with metadata attached\n",
    "    base_df = add_metadata_columns(df, \"quantum_base_processing\")\n",
    "\n",
    "    base_df = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"vqe_result.initial_data\").alias(\"initial_data\"),\n",
    "        col(\"vqe_result.iteration_list\").alias(\"iteration_list\"),\n",
    "        col(\"vqe_result.minimum\").alias(\"minimum_energy\"),\n",
    "        col(\"vqe_result.optimal_parameters\").alias(\"optimal_parameters\"),\n",
    "        col(\"vqe_result.maxcv\").alias(\"maxcv\"),\n",
    "        col(\"vqe_result.minimization_time\").alias(\"minimization_time\"),\n",
    "        col(\"hamiltonian_time\"),\n",
    "        col(\"mapping_time\"),\n",
    "        col(\"vqe_time\"),\n",
    "        col(\"total_time\"),\n",
    "        col(\"molecule.molecule_data\").alias(\"molecule_data\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    )\n",
    "\n",
    "    # molecule information dataframe\n",
    "    df_molecule = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"molecule_data.symbols\").alias(\"atom_symbols\"),\n",
    "        col(\"molecule_data.coords\").alias(\"coordinates\"),\n",
    "        col(\"molecule_data.multiplicity\").alias(\"multiplicity\"),\n",
    "        col(\"molecule_data.charge\").alias(\"charge\"),\n",
    "        col(\"molecule_data.units\").alias(\"coordinate_units\"),\n",
    "        col(\"molecule_data.masses\").alias(\"atomic_masses\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    )\n",
    "\n",
    "    # ansatz information dataframe\n",
    "    df_ansatz = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"initial_data.ansatz\").alias(\"ansatz\"),\n",
    "        col(\"initial_data.ansatz_reps\").alias(\"ansatz_reps\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    )\n",
    "\n",
    "    # metrics information dataframe\n",
    "    df_metrics = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"hamiltonian_time\"),\n",
    "        col(\"mapping_time\"),\n",
    "        col(\"vqe_time\"),\n",
    "        col(\"total_time\"),\n",
    "        col(\"minimization_time\"),\n",
    "        (col(\"hamiltonian_time\") + col(\"mapping_time\") + col(\"vqe_time\")).alias(\"computed_total_time\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    )\n",
    "\n",
    "    # VQE results dataframe\n",
    "    df_vqe = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"initial_data.backend\").alias(\"backend\"),\n",
    "        col(\"initial_data.num_qubits\").alias(\"num_qubits\"),\n",
    "        col(\"initial_data.optimizer\").alias(\"optimizer\"),\n",
    "        col(\"initial_data.noise_backend\").alias(\"noise_backend\"),\n",
    "        col(\"initial_data.default_shots\").alias(\"default_shots\"),\n",
    "        col(\"initial_data.ansatz_reps\").alias(\"ansatz_reps\"),\n",
    "        col(\"minimum_energy\"),\n",
    "        col(\"maxcv\"),\n",
    "        size(col(\"iteration_list\")).alias(\"total_iterations\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    )\n",
    "\n",
    "    # initial parameters dataframe\n",
    "    df_initial_parameters = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"initial_data.backend\").alias(\"backend\"),\n",
    "        col(\"initial_data.num_qubits\").alias(\"num_qubits\"),\n",
    "        explode(col(\"initial_data.initial_parameters\")).alias(\"initial_parameter_value\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).withColumn(\n",
    "        \"parameter_index\", \n",
    "        expr(\"hash(concat(experiment_id, initial_parameter_value)) % 1000000\")\n",
    "    ).withColumn(\n",
    "        \"parameter_id\", \n",
    "        expr(\"concat(experiment_id, '_init_', cast(parameter_index as string))\")\n",
    "    )\n",
    "\n",
    "    # optimal parameters\n",
    "    df_optimal_parameters = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"initial_data.backend\").alias(\"backend\"),\n",
    "        col(\"initial_data.num_qubits\").alias(\"num_qubits\"),\n",
    "        explode(col(\"optimal_parameters\")).alias(\"optimal_parameter_value\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).withColumn(\n",
    "        \"parameter_index\", \n",
    "        expr(\"hash(concat(experiment_id, optimal_parameter_value)) % 1000000\")\n",
    "    ).withColumn(\n",
    "        \"parameter_id\", \n",
    "        expr(\"concat(experiment_id, '_opt_', cast(parameter_index as string))\")\n",
    "    )\n",
    "\n",
    "    # iterations\n",
    "    df_iterations = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"initial_data.backend\").alias(\"backend\"),\n",
    "        col(\"initial_data.num_qubits\").alias(\"num_qubits\"),\n",
    "        explode(col(\"iteration_list\")).alias(\"iteration\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"backend\"),\n",
    "        col(\"num_qubits\"),\n",
    "        col(\"iteration.iteration\").alias(\"iteration_step\"),\n",
    "        col(\"iteration.result\").alias(\"iteration_energy\"),\n",
    "        col(\"iteration.std\").alias(\"energy_std_dev\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).withColumn(\n",
    "        \"iteration_id\", \n",
    "        expr(\"concat(experiment_id, '_iter_', cast(hash(concat(experiment_id, cast(iteration_step as string))) % 1000000 as string))\")\n",
    "    )\n",
    "\n",
    "    # iteration parameters\n",
    "    df_iteration_parameters = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"initial_data.backend\").alias(\"backend\"),\n",
    "        col(\"initial_data.num_qubits\").alias(\"num_qubits\"),\n",
    "        explode(col(\"iteration_list\")).alias(\"iteration\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"backend\"),\n",
    "        col(\"num_qubits\"),\n",
    "        col(\"iteration.iteration\").alias(\"iteration_step\"),\n",
    "        explode(col(\"iteration.parameters\")).alias(\"parameter_value\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).withColumn(\n",
    "        \"parameter_index\", \n",
    "        expr(\"hash(concat(experiment_id, cast(iteration_step as string), parameter_value)) % 1000000\")\n",
    "    ).withColumn(\n",
    "        \"iteration_id\", \n",
    "        expr(\"concat(experiment_id, '_iter_', cast(hash(concat(experiment_id, cast(iteration_step as string))) % 1000000 as string))\")\n",
    "    ).withColumn(\n",
    "        \"parameter_id\", \n",
    "        expr(\"concat(iteration_id, '_param_', cast(parameter_index as string))\")\n",
    "    )\n",
    "\n",
    "    # hamiltonian terms\n",
    "    df_hamiltonian = base_df.select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"initial_data.backend\").alias(\"backend\"),\n",
    "        explode(col(\"initial_data.hamiltonian\")).alias(\"hamiltonian_term\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).select(\n",
    "        col(\"experiment_id\"),\n",
    "        col(\"molecule_id\"),\n",
    "        col(\"basis_set\"),\n",
    "        col(\"backend\"),\n",
    "        col(\"hamiltonian_term.label\").alias(\"term_label\"),\n",
    "        col(\"hamiltonian_term.coefficients.real\").alias(\"coeff_real\"),\n",
    "        col(\"hamiltonian_term.coefficients.imaginary\").alias(\"coeff_imag\"),\n",
    "        col(\"processing_timestamp\"),\n",
    "        col(\"processing_date\"),\n",
    "        col(\"processing_batch_id\"),\n",
    "        col(\"processing_name\")\n",
    "    ).withColumn(\n",
    "        \"term_index\", \n",
    "        expr(\"hash(concat(experiment_id, term_label)) % 1000000\")\n",
    "    ).withColumn(\n",
    "        \"term_id\", \n",
    "        expr(\"concat(experiment_id, '_term_', cast(term_index as string))\")\n",
    "    )\n",
    "    \n",
    "    # return all the transformed dataframes\n",
    "    return {\n",
    "        \"molecules\": df_molecule,\n",
    "        \"ansatz_info\": df_ansatz,\n",
    "        \"performance_metrics\": df_metrics,\n",
    "        \"vqe_results\": df_vqe,\n",
    "        \"initial_parameters\": df_initial_parameters,\n",
    "        \"optimal_parameters\": df_optimal_parameters,\n",
    "        \"vqe_iterations\": df_iterations,\n",
    "        \"iteration_parameters\": df_iteration_parameters,\n",
    "        \"hamiltonian_terms\": df_hamiltonian,\n",
    "        \"base_df\": base_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e11e4-b570-41e2-ba30-8becca03a2fd",
   "metadata": {},
   "source": [
    "#### Store metadata in a separate table for tracking, in case it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a693b-3269-4d58-9d55-a3bd70c2527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_table_if_not_exists():\n",
    "    \"\"\"Create the metadata tracking table if it doesn't exist.\"\"\"\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS quantum_catalog.quantum_features.processing_metadata (\n",
    "        processing_batch_id STRING,\n",
    "        processing_name STRING,\n",
    "        processing_timestamp TIMESTAMP,\n",
    "        processing_date DATE,\n",
    "        table_names ARRAY<STRING>,\n",
    "        table_versions ARRAY<STRING>,\n",
    "        record_counts ARRAY<BIGINT>,\n",
    "        source_data_info STRING\n",
    "    ) USING iceberg\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea26885-943a-4eef-bc30-c93da5d2cc73",
   "metadata": {},
   "source": [
    "#### Update the metadata table with new processing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c426c41-176b-45d0-bb2f-dbebf70bcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata_table(dfs, table_names, table_versions, record_counts, source_info):\n",
    "    \"\"\"Update the metadata table with processing information.\"\"\"\n",
    "    base_df = dfs[\"base_df\"]\n",
    "    \n",
    "    processing_info = spark.createDataFrame([{\n",
    "        \"processing_batch_id\": base_df.first().processing_batch_id,\n",
    "        \"processing_name\": base_df.first().processing_name,\n",
    "        \"processing_timestamp\": base_df.first().processing_timestamp,\n",
    "        \"processing_date\": base_df.first().processing_date,\n",
    "        \"table_names\": table_names,\n",
    "        \"table_versions\": table_versions,\n",
    "        \"record_counts\": record_counts,\n",
    "        \"source_data_info\": source_info\n",
    "    }])\n",
    "\n",
    "    processing_info.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"quantum_catalog.quantum_features.processing_metadata\")\n",
    "    \n",
    "    print(f\"Updated metadata table with processing batch {base_df.first().processing_batch_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c63d4-cb01-484c-85a3-38b1e0f48cf8",
   "metadata": {},
   "source": [
    "### Process each experiment and update metadata accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2efe5-f1c4-4a11-a8a1-55ea9028e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiments_incrementally(df):\n",
    "    \"\"\"\n",
    "    Main function to process quantum data incrementally.\n",
    "    \n",
    "    Args:\n",
    "        df: Original dataframe with quantum simulation data\n",
    "    \"\"\"\n",
    "    # create metadata tracking table if it doesn't exist\n",
    "    create_metadata_table_if_not_exists()\n",
    "    \n",
    "    # transform the data\n",
    "    dfs = transform_quantum_data(df)\n",
    "    \n",
    "    # table configurations for each dataframe\n",
    "    table_configs = {\n",
    "        \"molecules\": {\n",
    "            \"key_columns\": [\"experiment_id\", \"molecule_id\"],\n",
    "            \"partition_columns\": [\"processing_date\"],\n",
    "            \"comment\": \"Molecule information for quantum simulations\"\n",
    "        },\n",
    "        \"ansatz_info\": {\n",
    "            \"key_columns\": [\"experiment_id\", \"molecule_id\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\"],\n",
    "            \"comment\": \"Ansatz configurations for quantum simulations\"\n",
    "        },\n",
    "        \"performance_metrics\": {\n",
    "            \"key_columns\": [\"experiment_id\", \"molecule_id\", \"basis_set\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\"],\n",
    "            \"comment\": \"Performance metrics for quantum simulations\"\n",
    "        },\n",
    "        \"vqe_results\": {\n",
    "            \"key_columns\": [\"experiment_id\", \"molecule_id\", \"basis_set\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\", \"backend\"],\n",
    "            \"comment\": \"VQE optimization results for quantum simulations\"\n",
    "        },\n",
    "        \"initial_parameters\": {\n",
    "            \"key_columns\": [\"parameter_id\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\"],\n",
    "            \"comment\": \"Initial parameters for VQE optimization\"\n",
    "        },\n",
    "        \"optimal_parameters\": {\n",
    "            \"key_columns\": [\"parameter_id\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\"],\n",
    "            \"comment\": \"Optimal parameters found by VQE optimization\"\n",
    "        },\n",
    "        \"vqe_iterations\": {\n",
    "            \"key_columns\": [\"iteration_id\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\", \"backend\"],\n",
    "            \"comment\": \"VQE optimization iterations and energy values\"\n",
    "        },\n",
    "        \"iteration_parameters\": {\n",
    "            \"key_columns\": [\"parameter_id\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\"],\n",
    "            \"comment\": \"Parameters at each iteration of VQE optimization\"\n",
    "        },\n",
    "        \"hamiltonian_terms\": {\n",
    "            \"key_columns\": [\"term_id\"],\n",
    "            \"partition_columns\": [\"processing_date\", \"basis_set\", \"backend\"],\n",
    "            \"comment\": \"Hamiltonian terms for quantum simulations\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # process each table incrementally\n",
    "    table_versions = []\n",
    "    record_counts = []\n",
    "    table_names = []\n",
    "    \n",
    "    for table_name, config in table_configs.items():\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "        version_tag, count = process_incremental_data(\n",
    "            dfs[table_name],\n",
    "            table_name,\n",
    "            config[\"key_columns\"],\n",
    "            config[\"partition_columns\"],\n",
    "            config[\"comment\"]\n",
    "        )\n",
    "        \n",
    "        table_names.append(table_name)\n",
    "        table_versions.append(version_tag if version_tag else \"no_changes\")\n",
    "        record_counts.append(count)\n",
    "    \n",
    "    # update metadata tracking\n",
    "    update_metadata_table(\n",
    "        dfs,\n",
    "        table_names,\n",
    "        table_versions,\n",
    "        record_counts,\n",
    "        \"Incremental VQE simulation data processing\"\n",
    "    )\n",
    "    \n",
    "    print(\"Incremental processing completed!\")\n",
    "    \n",
    "    # summary of processed records\n",
    "    return dict(zip(table_names, record_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474d754-6793-4f29-aa1c-d44bd46e2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_counts = process_experiments_incrementally(df)\n",
    "    \n",
    "print(\"\\nProcessing Summary:\")\n",
    "for table, count in processed_counts.items():\n",
    "    print(f\"{table}: {count} new records processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
