volumes:
  minio-data:
    name: quantum-minio-data
  spark-warehouse:
    name: quantum-spark-warehouse
  airflow-postgres-data:
    name: quantum-airflow-postgres
  airflow-logs:
    name: quantum-airflow-logs
  kafka-data:
    name: quantum-kafka-data

networks:
  quantum-net:
    name: quantum-pipeline-network
    driver: bridge

services:

  # CPU-only Quantum Pipeline for Thesis Comparison
  quantum-pipeline-cpu:
    container_name: quantum-pipeline-cpu
    command:
      - "--file"
      - "./data/molecules.json"
      - "--kafka"
      - "--topic"
      - "vqe_results_cpu"
      - "--max-iterations"
      - "${MAX_ITERATIONS:-100}"
      - "--log-level"
      - "${LOG_LEVEL:-INFO}"
      - "--simulation-method"
      - "${SIMULATION_METHOD:-statevector}"
      - "--convergence"
      - "--enable-performance-monitoring"
      - "--performance-interval"
      - "30"
      - "--performance-export-format"
      - "both"
    build:
      context: .
      dockerfile: ./docker/Dockerfile.cpu  # CPU-only dockerfile
    restart: always
    volumes:
      - ./gen/cpu:/usr/src/quantum-pipeline/gen/
      - ./data/molecules.json:/usr/src/quantum_pipeline/data/molecules.json
    environment:
      IBM_RUNTIME_CHANNEL: ${IBM_RUNTIME_CHANNEL}
      IBM_RUNTIME_INSTANCE: ${IBM_RUNTIME_INSTANCE}
      IBM_RUNTIME_TOKEN: ${IBM_RUNTIME_TOKEN}
      KAFKA_SERVERS: ${KAFKA_SERVERS:-kafka:9092}
      CONTAINER_TYPE: "CPU"
      QUANTUM_PERFORMANCE_ENABLED: "true"
      QUANTUM_PERFORMANCE_COLLECTION_INTERVAL: "30"
      QUANTUM_PERFORMANCE_PUSHGATEWAY_URL: "${EXTERNAL_PUSHGATEWAY_URL:-http://external-prometheus:9091}"
      QUANTUM_PERFORMANCE_EXPORT_FORMAT: "json,prometheus"
    networks:
      - quantum-net
    depends_on:
      kafka-connect-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: '3.0'        # Half of available CPU cores
          memory: 8G         # Conservative memory allocation
        reservations:
          cpus: '2.0'
          memory: 4G

  # GPU-accelerated Quantum Pipeline for Thesis Comparison
  quantum-pipeline-gpu:
    container_name: quantum-pipeline-gpu
    command:
      - "--file"
      - "./data/molecules.json"
      - "--kafka"
      - "--gpu"
      - "--topic"
      - "vqe_results_gpu"
      - "--max-iterations"
      - "${MAX_ITERATIONS:-100}"
      - "--log-level"
      - "${LOG_LEVEL:-INFO}"
      - "--simulation-method"
      - "${SIMULATION_METHOD:-statevector}"
      - "--convergence"
      - "--enable-performance-monitoring"
      - "--performance-interval"
      - "30"
      - "--performance-export-format"
      - "both"
    build:
      context: .
      dockerfile: ./docker/Dockerfile.gpu
    restart: always
    volumes:
      - ./gen/gpu:/usr/src/quantum-pipeline/gen/
      - ./data/molecules.json:/usr/src/quantum_pipeline/data/molecules.json
    environment:
      IBM_RUNTIME_CHANNEL: ${IBM_RUNTIME_CHANNEL}
      IBM_RUNTIME_INSTANCE: ${IBM_RUNTIME_INSTANCE}
      IBM_RUNTIME_TOKEN: ${IBM_RUNTIME_TOKEN}
      KAFKA_SERVERS: ${KAFKA_SERVERS:-kafka:9092}
      CONTAINER_TYPE: "GPU"
      QUANTUM_PERFORMANCE_ENABLED: "true"
      QUANTUM_PERFORMANCE_COLLECTION_INTERVAL: "30"
      QUANTUM_PERFORMANCE_PUSHGATEWAY_URL: "${EXTERNAL_PUSHGATEWAY_URL:-http://external-prometheus:9091}"
      QUANTUM_PERFORMANCE_EXPORT_FORMAT: "json,prometheus"
      CUDA_VISIBLE_DEVICES: "0"  # Use the more powerful GTX 1060
    networks:
      - quantum-net
    depends_on:
      kafka-connect-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: '3.0'        # Same CPU limit as CPU container for fair comparison
          memory: 8G         # Same memory limit for fair comparison
        reservations:
          cpus: '2.0'
          memory: 4G
          devices:
            - driver: nvidia
              device_ids: ['0']  # GTX 1060 6GB specifically
              capabilities: [gpu]

  schema-registry:
    image: confluentinc/cp-schema-registry:${SCHEMA_REGISTRY_VERSION:-latest}
    container_name: schema-registry
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "${KAFKA_SERVERS}"
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: "${SCHEMA_REGISTRY_TOPIC}"
      SCHEMA_REGISTRY_HOST_NAME: "${SCHEMA_REGISTRY_HOSTNAME:-schema-registry}"
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
    ports:
      - "${SCHEMA_REGISTRY_PORT:-8081}:8081"
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "curl", "--fail", "-s", "http://localhost:8081/subjects"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 15s

  kafka:
    image: bitnami/kafka:${KAFKA_VERSION:-latest}
    container_name: kafka
    restart: unless-stopped
    ports:
      - "${KAFKA_EXTERNAL_PORT:-9094}:9094"
      - "${KAFKA_INTERNAL_PORT:-9092}:9092"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093

      - KAFKA_CFG_LISTENERS=PLAINTEXT://kafka:9092,CONTROLLER://:9093,EXTERNAL://0.0.0.0:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://${KAFKA_EXTERNAL_HOST_IP:-localhost}:${KAFKA_EXTERNAL_PORT:-9094}
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CLIENT_LISTENER_NAME=PLAINTEXT
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server kafka:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    volumes:
      - kafka-data:/bitnami/kafka

  kafka-connect:
    image: confluentinc/cp-kafka-connect:${KAFKA_CONNECT_VERSION:-latest}
    container_name: kafka-connect
    restart: unless-stopped
    ports:
      - "${KAFKA_CONNECT_PORT:-8083}:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "${KAFKA_SERVERS}"
      CONNECT_REST_PORT: 8083

      CONNECT_GROUP_ID: "connect-cluster"

      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"

      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"

      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"

      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"

      CONNECT_LOG4J_ROOT_LOGLEVEL: "${KAFKA_CONNECT_LOG_LEVEL:-INFO}"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"

      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"

      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"

      AWS_ACCESS_KEY: "${MINIO_ACCESS_KEY}"
      AWS_SECRET_KEY: "${MINIO_SECRET_KEY}"
    command:
      - bash
      - -c
      - |
        echo "Installing connector plugins"
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:latest
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run
    networks:
      - quantum-net
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      minio:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  kafka-connect-init:
    image: curlimages/curl
    container_name: kafka-connect-init
    depends_on:
      kafka-connect:
        condition: service_healthy
    entrypoint: >
      sh -c "
        echo 'Waiting for Kafka Connect...';
        until curl -s -o /dev/null http://kafka-connect:8083/connectors;
        do
            sleep 5;
        done;
        sleep 10;

        echo 'Registering MinIO Sink Connector...';
        for i in {1..5}; do
          if curl -X POST -H 'Content-Type: application/json' \
            --data @/connectors/minio-sink-config.json \
            http://kafka-connect:8083/connectors; then
            echo 'MinIO Sink Connector registered successfully!';
            break;
          else
            echo 'Failed to register MinIO connector, retrying in 5 seconds...';
            sleep 5;
            if [ $i -eq 5 ]; then
              echo 'Failed to register MinIO connector after 5 attempts';
              exit 1;
            fi
          fi
        done;
      "
    volumes:
      - ./docker/connectors/minio-sink-config.json:/connectors/minio-sink-config.json
    networks:
      - quantum-net

  minio:
    image: minio/minio:${MINIO_VERSION:-latest}
    container_name: minio
    ports:
      - "${MINIO_API_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_REGION_NAME: ${MINIO_REGION}
      MINIO_BROWSER_REDIRECT_URL: http://localhost:${MINIO_CONSOLE_PORT}
    command: minio server /data --console-address ":${MINIO_CONSOLE_PORT}"
    volumes:
      - minio-data:/data
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_API_PORT}/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  mc-setup:
    image: minio/mc
    container_name: mc-setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c " /usr/bin/mc config host add ${MINIO_HOSTNAME} http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};

      /usr/bin/mc ls ${MINIO_HOSTNAME}/iceberg || /usr/bin/mc mb ${MINIO_HOSTNAME}/iceberg;

      /usr/bin/mc ls ${MINIO_HOSTNAME}/${MINIO_BUCKET} || /usr/bin/mc mb ${MINIO_HOSTNAME}/${MINIO_BUCKET};

      /usr/bin/mc policy set public ${MINIO_HOSTNAME}/iceberg; /usr/bin/mc policy set public ${MINIO_HOSTNAME}/${MINIO_BUCKET};

      exit 0; "
    networks:
      - quantum-net

  spark-master:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_MASTER_OPTS="-Dspark.serializer=org.apache.spark.serializer.KryoSerializer -Dspark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions -Dspark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog -Dspark.sql.catalog.spark_catalog.type=hive -Dspark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog -Dspark.sql.catalog.iceberg.type=hadoop -Dspark.sql.catalog.iceberg.warehouse=s3://iceberg/warehouse -Dspark.sql.catalog.iceberg.s3.endpoint=http://minio:9000 -Dspark.sql.catalog.iceberg.s3.path-style-access=true -Dspark.hadoop.fs.s3a.access.key=${MINIO_ACCESS_KEY} -Dspark.hadoop.fs.s3a.secret.key=${MINIO_SECRET_KEY} -Dspark.hadoop.fs.s3a.endpoint=minio:9000 -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -Dspark.hadoop.fs.s3a.path.style.access=true -Dspark.hadoop.fs.s3a.connection.ssl.enabled=false -Dspark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - spark-warehouse:/opt/bitnami/spark/warehouse
    networks:
      - quantum-net
    depends_on:
      - minio

  spark-worker:
    container_name: spark-worker
    build:
      context: .
      dockerfile: ./docker/Dockerfile.spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G          # Increased for better data processing
      - SPARK_WORKER_CORES=2            # Use remaining CPU cores
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_WORKER_OPTS="-Dspark.serializer=org.apache.spark.serializer.KryoSerializer -Dspark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions -Dspark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog -Dspark.sql.catalog.spark_catalog.type=hive -Dspark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog -Dspark.sql.catalog.iceberg.type=hadoop -Dspark.sql.catalog.iceberg.warehouse=s3://iceberg/warehouse -Dspark.sql.catalog.iceberg.s3.endpoint=http://minio:9000 -Dspark.sql.catalog.iceberg.s3.path-style-access=true -Dspark.hadoop.fs.s3a.access.key=${MINIO_ACCESS_KEY} -Dspark.hadoop.fs.s3a.secret.key=${MINIO_SECRET_KEY} -Dspark.hadoop.fs.s3a.endpoint=minio:9000 -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -Dspark.hadoop.fs.s3a.path.style.access=true -Dspark.hadoop.fs.s3a.connection.ssl.enabled=false -Dspark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    volumes:
      - spark-warehouse:/opt/bitnami/spark/warehouse
    depends_on:
      - spark-master
    networks:
      - quantum-net
    deploy:
      resources:
        limits:
          cpus: '1.0'      # Reserve 1 core for Spark to avoid interference
          memory: 4G       # Reduced memory allocation
        reservations:
          cpus: '0.5'
          memory: 2G


  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${AIRFLOW_POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${AIRFLOW_POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${AIRFLOW_POSTGRES_DB:-airflow}
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${AIRFLOW_POSTGRES_USER:-airflow}"]
      interval: 5s
      retries: 5

  airflow-init:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.airflow
    image: apache/airflow:2.7.1
    container_name: airflow-init
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW_DAGS_PAUSED_AT_CREATION:-True}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW_LOAD_EXAMPLES:-False}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_WEBSERVER_SECRET_KEY}
    volumes:
      - ./docker/airflow/:/opt/airflow/dags
    command: version
    entrypoint: >
      /bin/bash -c "
        airflow db upgrade
        airflow db migrate

        airflow users create --username ${AIRFLOW_ADMIN_USERNAME:-admin} --password ${AIRFLOW_ADMIN_PASSWORD:-admin} --firstname ${AIRFLOW_ADMIN_FIRSTNAME:-Admin} --lastname ${AIRFLOW_ADMIN_LASTNAME:-User} --role Admin --email ${AIRFLOW_ADMIN_EMAIL:-admin@example.com}

        airflow connections add 'spark_default' --conn-type 'spark' --conn-host 'spark://${SPARK_MASTER_HOST:-spark-master}:${SPARK_MASTER_PORT:-7077}' --conn-extra '{\"queue\": \"${SPARK_DEFAULT_QUEUE:-default}\"}'"
    networks:
      - quantum-net

  airflow-webserver:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.airflow
    container_name: airflow-webserver
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW_DAGS_PAUSED_AT_CREATION:-True}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW_LOAD_EXAMPLES:-False}
      - AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
      - AIRFLOW__LOGGING__REMOTE_LOGGING=False
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_WEBSERVER_SECRET_KEY}
    volumes:
      - ./docker/airflow/:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8084}:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    networks:
      - quantum-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 2G
        reservations:
          cpus: '0.2'
          memory: 1G

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.airflow
    container_name: airflow-scheduler
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW_DAGS_PAUSED_AT_CREATION:-True}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW_LOAD_EXAMPLES:-False}
      - AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
      - AIRFLOW__LOGGING__REMOTE_LOGGING=False
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_WEBSERVER_SECRET_KEY}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
    volumes:
      - ./docker/airflow/:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    command: scheduler
    networks:
      - quantum-net
    restart: always

  airflow-triggerer:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.airflow
    container_name: airflow-triggerer
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW_DAGS_PAUSED_AT_CREATION:-True}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW_LOAD_EXAMPLES:-False}
      - AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
      - AIRFLOW__LOGGING__REMOTE_LOGGING=False
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_WEBSERVER_SECRET_KEY}
    volumes:
      - ./docker/airflow/:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    command: triggerer
    networks:
      - quantum-net
    restart: always

  # Monitoring Agents for External Connection
  dozzle:
    image: amir20/dozzle:latest
    container_name: dozzle-agent
    command: agent
    ports:
      - "7007:7007"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped
    networks:
      - quantum-net

  portainer:
    image: portainer/agent:latest
    container_name: portainer-agent
    restart: unless-stopped
    ports:
      - "9001:9001"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/run/docker/volumes:/var/lib/docker/volumes:ro
    networks:
      - quantum-net
