volumes:
  minio-data:
    name: quantum-minio-data
  spark-warehouse:
    name: quantum-spark-warehouse
  metastore-db:
    name: quantum-metastore-db

networks:
  quantum-net:
    name: quantum-pipeline-network
    driver: bridge

services:

  quantum-pipeline:
    container_name: quantum-pipeline
    command:
      - "--file"
      - "data/molecules.json"
      - "--kafka"
      - "--gpu"
      - "--max-iterations"
      - "${MAX_ITERATIONS}"
      - "--log-level"
      - "${LOG_LEVEL:-INFO}"
      - "--simulation-method"
      - "${SIMULATION_METHOD:-statevector}"
    build:
      context: .
      dockerfile: ./docker/Dockerfile.gpu
    restart: always
    volumes:
      - ./gen/:/usr/src/quantum-pipeline/gen/
    environment:
      IBM_RUNTIME_CHANNEL: ${IBM_RUNTIME_CHANNEL}
      IBM_RUNTIME_INSTANCE: ${IBM_RUNTIME_INSTANCE}
      IBM_RUNTIME_TOKEN: ${IBM_RUNTIME_TOKEN}
      KAFKA_SERVERS: ${KAFKA_SERVERS:-kafka:9092}
    networks:
      - quantum-net
    depends_on:
      kafka-connect-init:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  schema-registry:
    image: confluentinc/cp-schema-registry:${SCHEMA_REGISTRY_VERSION:-latest}
    container_name: schema-registry
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "${KAFKA_SERVERS}"
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: "${SCHEMA_REGISTRY_TOPIC}"
      SCHEMA_REGISTRY_HOST_NAME: "${SCHEMA_REGISTRY_HOSTNAME:-schema-registry}"
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
    ports:
      - "${SCHEMA_REGISTRY_PORT:-8081}:8081"
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "curl", "--fail", "-s", "http://localhost:8081/subjects"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 15s

  kafka:
    image: bitnami/kafka:${KAFKA_VERSION:-latest}
    container_name: kafka
    restart: unless-stopped
    ports:
      - "${KAFKA_EXTERNAL_PORT:-9094}:9094"
      - "${KAFKA_INTERNAL_PORT:-9092}:9092"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093

      - KAFKA_CFG_LISTENERS=PLAINTEXT://kafka:9092,CONTROLLER://:9093,EXTERNAL://0.0.0.0:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://${KAFKA_EXTERNAL_HOST_IP:-localhost}:${KAFKA_EXTERNAL_PORT:-9094}
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CLIENT_LISTENER_NAME=PLAINTEXT
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server kafka:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  kafka-connect:
    image: confluentinc/cp-kafka-connect:${KAFKA_CONNECT_VERSION:-latest}
    container_name: kafka-connect
    restart: unless-stopped
    ports:
      - "${KAFKA_CONNECT_PORT:-8083}:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "${KAFKA_SERVERS}"
      CONNECT_REST_PORT: 8083

      CONNECT_GROUP_ID: "connect-cluster"

      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"

      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"

      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"

      CONNECT_LOG4J_ROOT_LOGLEVEL: "${KAFKA_CONNECT_LOG_LEVEL:-INFO}"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"

      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components,/connectors"

      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"

      AWS_ACCESS_KEY: "${MINIO_ACCESS_KEY}"
      AWS_SECRET_KEY: "${MINIO_SECRET_KEY}"
    command:
      - bash
      - -c
      - |
        echo "Installing connector plugins"
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:latest
        confluent-hub install --no-prompt tabular/iceberg-kafka-connect:latest
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run
    networks:
      - quantum-net
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      minio:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  kafka-connect-init:
    image: curlimages/curl
    container_name: kafka-connect-init
    depends_on:
      kafka-connect:
        condition: service_healthy
    entrypoint: >
      sh -c "
        echo 'Waiting for Kafka Connect...';
        until curl -s -o /dev/null http://kafka-connect:8083/connectors; do
          sleep 5;
        done;
        # Add a buffer to ensure Kafka Connect is fully ready
        sleep 10;

        echo 'Registering MinIO Sink Connector...';
        for i in {1..5}; do
          if curl -X POST -H 'Content-Type: application/json' --data @/connectors/minio-sink-config.json http://kafka-connect:8083/connectors;
      then
            echo 'MinIO Sink Connector registered successfully!';
            break;
          else
            echo 'Failed to register MinIO connector, retrying in 5 seconds...';
            sleep 5;
            if [ $i -eq 5 ]; then
              echo 'Failed to register MinIO connector after 5 attempts';
              exit 1;
            fi
          fi
        done;

        echo 'Registering Iceberg Sink Connector...';
        for i in {1..5}; do
          if curl -X POST -H 'Content-Type: application/json' --data @/connectors/iceberg-sink-config.json http://kafka-connect:8083/connectors;
      then
            echo 'Iceberg Sink Connector registered successfully!';
            exit 0;
          else
            echo 'Failed to register Iceberg connector, retrying in 5 seconds...';
            sleep 5;
            if [ $i -eq 5 ]; then
              echo 'Failed to register Iceberg connector after 5 attempts';
              exit 1;
            fi
          fi
        done;
      "
    volumes:
      - ./docker/connectors/minio-sink-config.json:/connectors/minio-sink-config.json
      - ./docker/connectors/iceberg-sink-config.json:/connectors/iceberg-sink-config.json
    networks:
      - quantum-net

  minio:
    image: minio/minio:${MINIO_VERSION:-latest}
    container_name: minio
    ports:
      - "${MINIO_API_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_REGION_NAME: ${MINIO_REGION}
      MINIO_BROWSER_REDIRECT_URL: http://localhost:${MINIO_CONSOLE_PORT}
    command: minio server /data --console-address ":${MINIO_CONSOLE_PORT}"
    volumes:
      - minio-data:/data
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_API_PORT}/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  mc-setup:
    image: minio/mc
    container_name: mc-setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c " /usr/bin/mc config host add ${MINIO_HOSTNAME} http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};

      /usr/bin/mc ls ${MINIO_HOSTNAME}/iceberg || /usr/bin/mc mb ${MINIO_HOSTNAME}/iceberg;

      /usr/bin/mc ls ${MINIO_HOSTNAME}/${MINIO_BUCKET} || /usr/bin/mc mb ${MINIO_HOSTNAME}/${MINIO_BUCKET};

      /usr/bin/mc policy set public ${MINIO_HOSTNAME}/iceberg; /usr/bin/mc policy set public ${MINIO_HOSTNAME}/${MINIO_BUCKET};

      exit 0; "
    networks:
      - quantum-net

  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: iceberg-rest
    depends_on:
      minio:
        condition: service_started
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY}
      AWS_REGION: ${MINIO_REGION}
      CATALOG_WAREHOUSE: s3://iceberg/warehouse
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH_STYLE_ACCESS: "true"
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/v1/namespaces"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    ports:
      - "9083:9083"
    environment:
      - DB_DRIVER=org.postgresql.Driver
      - DB_USER=hive
      - DB_PASS=hive
      - DB_URL=jdbc:postgresql://postgres:5432/metastore
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - S3_ENDPOINT=http://minio:9000
      - S3_PATH_STYLE_ACCESS=true
    volumes:
      - metastore-db:/var/lib/hive
    depends_on:
      - postgres
    networks:
      - quantum-net

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_DB=metastore
    volumes:
      - metastore-db:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_OPTS="-Dspark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions -Dspark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
        -Dspark.sql.catalog.spark_catalog.type=hive -Dspark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog -Dspark.sql.catalog.iceberg.type=rest
        -Dspark.sql.catalog.iceberg.uri=http://iceberg-rest:8181 -Dspark.sql.catalog.iceberg.warehouse=s3://iceberg/warehouse
        -Dspark.sql.catalog.iceberg.s3.endpoint=http://minio:9000 -Dspark.sql.catalog.iceberg.s3.path-style-access=true -Dspark.hadoop.fs.s3a.access.key=${MINIO_ACCESS_KEY}
        -Dspark.hadoop.fs.s3a.secret.key=${MINIO_SECRET_KEY} -Dspark.hadoop.fs.s3a.endpoint=minio:9000 -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
        -Dspark.hadoop.fs.s3a.path.style.access=true -Dspark.hadoop.fs.s3a.connection.ssl.enabled=false"
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - spark-warehouse:/opt/bitnami/spark/warehouse
    networks:
      - quantum-net
    depends_on:
      - minio
      - iceberg-rest

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_OPTS="-Dspark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions -Dspark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
        -Dspark.sql.catalog.spark_catalog.type=hive -Dspark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog -Dspark.sql.catalog.iceberg.type=rest
        -Dspark.sql.catalog.iceberg.uri=http://iceberg-rest:8181 -Dspark.sql.catalog.iceberg.warehouse=s3://iceberg/warehouse
        -Dspark.sql.catalog.iceberg.s3.endpoint=http://minio:9000 -Dspark.sql.catalog.iceberg.s3.path-style-access=true -Dspark.hadoop.fs.s3a.access.key=${MINIO_ACCESS_KEY}
        -Dspark.hadoop.fs.s3a.secret.key=${MINIO_SECRET_KEY} -Dspark.hadoop.fs.s3a.endpoint=minio:9000 -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
        -Dspark.hadoop.fs.s3a.path.style.access=true -Dspark.hadoop.fs.s3a.connection.ssl.enabled=false"
    volumes:
      - spark-warehouse:/opt/bitnami/spark/warehouse
    depends_on:
      - spark-master
    networks:
      - quantum-net

  spark-init:
    image: bitnami/spark:latest
    container_name: spark-init
    command:
      - /bin/bash
      - -c
      - |
        echo "Creating Iceberg tables..."
        /opt/bitnami/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --conf "spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions" \
          --conf "spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog" \
          --conf "spark.sql.catalog.spark_catalog.type=hive" \
          --conf "spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog" \
          --conf "spark.sql.catalog.iceberg.type=rest" \
          --conf "spark.sql.catalog.iceberg.uri=http://iceberg-rest:8181" \
          --conf "spark.sql.catalog.iceberg.warehouse=s3://iceberg/warehouse" \
          --conf "spark.sql.catalog.iceberg.s3.endpoint=http://minio:9000" \
          --conf "spark.sql.catalog.iceberg.s3.path-style-access=true" \
          --conf "spark.hadoop.fs.s3a.access.key=${MINIO_ACCESS_KEY}" \
          --conf "spark.hadoop.fs.s3a.secret.key=${MINIO_SECRET_KEY}" \
          --conf "spark.hadoop.fs.s3a.endpoint=minio:9000" \
          --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
          --conf "spark.hadoop.fs.s3a.path.style.access=true" \
          --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
          --class org.apache.spark.sql.SparkSession \
          --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,org.apache.hadoop:hadoop-aws:3.3.4 \
          /opt/bitnami/spark/examples/jars/spark-examples_2.12-3.5.0.jar \
          "CREATE NAMESPACE IF NOT EXISTS iceberg.quantum; \
           CREATE TABLE IF NOT EXISTS iceberg.quantum.simulation_results \
           ( \
             id BIGINT, \
             molecule STRUCT<molecule_data: STRUCT<symbols: ARRAY<STRING>, coords: ARRAY<ARRAY<DOUBLE>>, multiplicity: INT, charge: INT, units: STRING, masses: ARRAY<DOUBLE>>>, \
             basis_set STRING, \
             hamiltonian_time DOUBLE, \
             mapping_time DOUBLE, \
             vqe_time DOUBLE, \
             total_time DOUBLE, \
             vqe_result STRUCT<initial_data: STRUCT<backend: STRING, num_qubits: INT, hamiltonian: ARRAY<STRUCT<label: STRING, coefficients: STRUCT<real: DOUBLE, imaginary: DOUBLE>>>, num_parameters: INT, initial_parameters: ARRAY<DOUBLE>, optimizer: STRING, ansatz: STRING, noise_backend: STRING, default_shots: INT, ansatz_reps: INT>, \
                              iteration_list: ARRAY<STRUCT<iteration: INT, parameters: ARRAY<DOUBLE>, result: DOUBLE, std: DOUBLE>>, \
                              minimum: DOUBLE, \
                              optimal_parameters: ARRAY<DOUBLE>, \
                              maxcv: DOUBLE> \
           ) \
           USING iceberg \
           PARTITIONED BY (basis_set); \
           CREATE TEMPORARY VIEW raw_vqe_data \
           USING json \
           OPTIONS ( \
             path 's3a://${MINIO_BUCKET}/*', \
             multiLine 'true' \
           ); \
           INSERT INTO iceberg.quantum.simulation_results \
           SELECT id, molecule, basis_set, hamiltonian_time, mapping_time, vqe_time, total_time, vqe_result \
           FROM raw_vqe_data;"
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - AWS_REGION=${MINIO_REGION}
    volumes:
      - spark-warehouse:/opt/bitnami/spark/warehouse
    depends_on:
      - spark-master
      - spark-worker
      - minio
      - iceberg-rest
    networks:
      - quantum-net
