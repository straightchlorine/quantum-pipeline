volumes:
  minio-data:
  spark-warehouse:
  metastore-db:

networks:
  quantum-net:
    name: quantum-pipeline-network
    driver: bridge

services:

  quantum-pipeline:
    container_name: quantum-pipeline
    command:
      - "--file"
      - "data/molecules.json"
      - "--kafka"
      - "--gpu"
      - "--max-iterations"
      - "${MAX_ITERATIONS}"
      - "--log-level"
      - "${LOG_LEVEL:-INFO}"
      - "--simulation-method"
      - "${SIMULATION_METHOD:-statevector}"
    build:
      context: .
      dockerfile: ./docker/Dockerfile.gpu
    restart: always
    volumes:
      - ./gen/:/usr/src/quantum-pipeline/gen/
    environment:
      IBM_RUNTIME_CHANNEL: ${IBM_RUNTIME_CHANNEL}
      IBM_RUNTIME_INSTANCE: ${IBM_RUNTIME_INSTANCE}
      IBM_RUNTIME_TOKEN: ${IBM_RUNTIME_TOKEN}
      KAFKA_SERVERS: ${KAFKA_SERVERS:-kafka:9092}
    networks:
      - quantum-net
    depends_on:
      kafka-connect-init:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  schema-registry:
    image: confluentinc/cp-schema-registry:${SCHEMA_REGISTRY_VERSION:-latest}
    container_name: schema-registry
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "${KAFKA_SERVERS}"
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: "${SCHEMA_REGISTRY_TOPIC}"
      SCHEMA_REGISTRY_HOST_NAME: "${SCHEMA_REGISTRY_HOSTNAME:-schema-registry}"
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
    ports:
      - "${SCHEMA_REGISTRY_PORT:-8081}:8081"
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "curl", "--fail", "-s", "http://localhost:8081/subjects"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 15s

  kafka:
    image: bitnami/kafka:${KAFKA_VERSION:-latest}
    container_name: kafka
    restart: unless-stopped
    ports:
      - "${KAFKA_EXTERNAL_PORT:-9094}:9094"
      - "${KAFKA_INTERNAL_PORT:-9092}:9092"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093

      - KAFKA_CFG_LISTENERS=PLAINTEXT://kafka:9092,CONTROLLER://:9093,EXTERNAL://0.0.0.0:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://${KAFKA_EXTERNAL_HOST_IP:-localhost}:${KAFKA_EXTERNAL_PORT:-9094}
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CLIENT_LISTENER_NAME=PLAINTEXT
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server kafka:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  kafka-connect:
    image: confluentinc/cp-kafka-connect:${KAFKA_CONNECT_VERSION:-latest}
    container_name: kafka-connect
    restart: unless-stopped
    ports:
      - "${KAFKA_CONNECT_PORT:-8083}:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "${KAFKA_SERVERS}"
      CONNECT_REST_PORT: 8083

      CONNECT_GROUP_ID: "connect-cluster"

      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"

      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"

      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"

      CONNECT_LOG4J_ROOT_LOGLEVEL: "${KAFKA_CONNECT_LOG_LEVEL:-INFO}"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"

      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components,/connectors"

      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"

      AWS_ACCESS_KEY: "${MINIO_ACCESS_KEY}"
      AWS_SECRET_KEY: "${MINIO_SECRET_KEY}"
    command:
      - bash
      - -c
      - |
        echo "Installing connector plugins"
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:latest
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run
    networks:
      - quantum-net
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      minio:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  kafka-connect-init:
    image: curlimages/curl
    container_name: kafka-connect-init
    depends_on:
      kafka-connect:
        condition: service_healthy
    entrypoint: >
      bash -c "
        echo 'Waiting for Kafka Connect...';
        until curl -s -o /dev/null http://kafka-connect:8083/connectors; do
          sleep 5;
        done;
        echo 'Registering MinIO Sink Connector...';
        curl -X POST -H 'Content-Type: application/json' --data @/connectors/minio-sink-config.json http://kafka-connect:8083/connectors;
        echo 'MinIO Sink Connector registered!';
        exit 0;
      "
    volumes:
      - ./docker/connectors/minio-sink-config.json:/connectors/minio-sink-config.json
    networks:
      - quantum-net

  minio:
    image: minio/minio:${MINIO_VERSION:-latest}
    container_name: minio
    ports:
      - "${MINIO_API_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_REGION_NAME: ${MINIO_REGION}
      MINIO_BROWSER_REDIRECT_URL: http://localhost:${MINIO_CONSOLE_PORT}
    command: minio server /data --console-address ":${MINIO_CONSOLE_PORT}"
    volumes:
      - minio-data:/data
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_API_PORT}/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  mc-setup:
    image: minio/mc
    container_name: mc-setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        /usr/bin/mc config host add ${MINIO_HOSTNAME} http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
        /usr/bin/mc ls ${MINIO_HOSTNAME}/${MINIO_BUCKET} || /usr/bin/mc mb ${MINIO_HOSTNAME}/${MINIO_BUCKET};
        /usr/bin/mc policy set public ${MINIO_HOSTNAME}/${MINIO_BUCKET};
        exit 0;
      "
    networks:
      - quantum-net

  postgres:
    image: postgres:15
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: "metastore"
      POSTGRES_USER: "${POSTGRES_USER:-hive}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD:-hive}"
    volumes:
      - metastore-db:/var/lib/postgresql/data
    networks:
      - quantum-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-hive}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  hive-metastore:
    image: apache/hive:${APACHE_HIVE_VERSION:-4.0.1}
    container_name: hive-metastore
    restart: unless-stopped
    environment:
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://postgres:5432/metastore"
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: "org.postgresql.Driver"
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: "${POSTGRES_USER:-hive}"
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: "${POSTGRES_PASSWORD:-hive}"
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: "true"
      HIVE_SITE_CONF_hive_metastore_uris: "thrift://hive-metastore:9083"
      HIVE_SITE_CONF_fs_s3a_endpoint: "http://minio:9000"
      HIVE_SITE_CONF_fs_s3a_access_key: "${MINIO_ROOT_USER}"
      HIVE_SITE_CONF_fs_s3a_secret_key: "${MINIO_ROOT_PASSWORD}"
      HIVE_SITE_CONF_fs_s3a_path_style_access: "true"
      HIVE_SITE_CONF_fs_s3a_impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    command: ["hiveserver2"]
    ports:
      - "${HIVE_METASTORE_PORT:-9083}:9083"
    networks:
      - quantum-net
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      kafka-connect:
        condition: service_healthy

  spark-master:
    image: bitnami/spark:${SPARK_VERSION:-3.5.5}
    container_name: spark-master
    restart: unless-stopped
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - |
        SPARK_MASTER_OPTS=
        -Dspark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.apache.spark.sql.delta.catalog.DeltaSparkSessionExtension
        -Dspark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
        -Dspark.sql.catalog.iceberg.type=hive
        -Dspark.sql.catalog.iceberg.uri=thrift://hive-metastore:9083
        -Dspark.sql.catalog.iceberg.warehouse=s3a://${MINIO_BUCKET}/warehouse
        -Dspark.sql.catalog.delta=org.apache.spark.sql.delta.catalog.DeltaCatalog
        -Dspark.hadoop.fs.s3a.endpoint=http://minio:9000
        -Dspark.hadoop.fs.s3a.access.key=${MINIO_ROOT_USER}
        -Dspark.hadoop.fs.s3a.secret.key=${MINIO_ROOT_PASSWORD}
        -Dspark.hadoop.fs.s3a.path.style.access=true
        -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
        -Dspark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    ports:
      - "${SPARK_MASTER_UI_PORT:-8080}:8080"
    networks:
      - quantum-net

  spark-worker:
    image: bitnami/spark:${SPARK_VERSION:-3.5.5}
    container_name: spark-worker
    restart: unless-stopped
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-4G}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-2}
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - |
        SPARK_WORKER_OPTS=
        -Dspark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.apache.spark.sql.delta.catalog.DeltaSparkSessionExtension
        -Dspark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
        -Dspark.sql.catalog.iceberg.type=hive
        -Dspark.sql.catalog.iceberg.uri=thrift://hive-metastore:9083
        -Dspark.sql.catalog.iceberg.warehouse=s3a://${MINIO_BUCKET}/warehouse
        -Dspark.sql.catalog.delta=org.apache.spark.sql.delta.catalog.DeltaCatalog
        -Dspark.hadoop.fs.s3a.endpoint=http://minio:9000
        -Dspark.hadoop.fs.s3a.access.key=${MINIO_ROOT_USER}
        -Dspark.hadoop.fs.s3a.secret.key=${MINIO_ROOT_PASSWORD}
        -Dspark.hadoop.fs.s3a.path.style.access=true
        -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
        -Dspark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    ports:
      - "${SPARK_WORKER_UI_PORT:-8086}:8081"
    networks:
      - quantum-net
    depends_on:
      - spark-master
    deploy:
      resources:
        limits:
          cpus: '${SPARK_WORKER_CORES:-2}'
          memory: ${SPARK_WORKER_MEMORY:-4G}
